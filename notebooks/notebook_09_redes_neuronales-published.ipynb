{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwVyqxGaybTo"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_09_redes_neuronales-published.ipynb)\n",
        "\n",
        "# Redes neuronales\n",
        "\n",
        "\n",
        "Vamos nuevamente a trabajar con los datos de `iris` para entrenar (y antes construir) una Red Neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cqHhDZmSybTy",
        "outputId": "5b408bfe-766a-4e76-9f70-a46385bc26c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_data():\n",
        "    dataset = load_iris()\n",
        "    X = dataset[\"data\"]\n",
        "    y = dataset[\"target\"]\n",
        "    y = LabelEncoder().fit_transform(y)\n",
        "    return np.array(X), np.array(y)\n",
        "X, y = get_data()\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "id": "xoCRusfIUKaG",
        "outputId": "62098a08-2c22-47e7-e54e-3594cb0de91c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(0)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSPzqQhSybT5"
      },
      "source": [
        "La propuesta es empezar por el esqueleto de las 2 clases que usaremos para esta tarea e ir implementado los métodos a medida que avancemos.\n",
        "\n",
        "Al final de este notebook se encuentran ambas clases completas. Pueden copiar el código desde allí mismo o implementarlo. La idea es que en cada avance podamos comprender la parte del proceso que estamos realizando, por lo cual se recomienda seguir la guia propuesta e ir completando sólo lo que es necesario para cada punto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GViXXRmWybT7"
      },
      "outputs": [],
      "source": [
        "class Capa:\n",
        "    def __init__(self, neuronas):\n",
        "        self.neuronas = neuronas\n",
        "\n",
        "    def forward(self, inputs, weights, bias, activation):\n",
        "        \"\"\"\n",
        "        Forward Propagation de la capa\n",
        "        \"\"\"\n",
        "        Z_curr = np.dot(inputs, weights.T) + bias\n",
        "\n",
        "        if activation == 'relu':\n",
        "            A_curr = self.relu(inputs=Z_curr)\n",
        "        elif activation == 'softmax':\n",
        "            A_curr = self.softmax(inputs=Z_curr)\n",
        "\n",
        "        return A_curr, Z_curr #Z son las preactivaciones (variables salidas del producto matricial con los pesos) y A las activaciones\n",
        "\n",
        "    def relu(self, inputs):\n",
        "        \"\"\"\n",
        "        ReLU: función de activación\n",
        "        \"\"\"\n",
        "\n",
        "        return np.maximum(0, inputs)\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        \"\"\"\n",
        "        Softmax: función de activación\n",
        "        \"\"\"\n",
        "        exp_scores = np.exp(inputs)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
        "        \"\"\"\n",
        "        Backward Propagation de la capa\n",
        "        \"\"\"\n",
        "        if activation == 'softmax':\n",
        "            dW = np.dot(A_prev.T, dA_curr)\n",
        "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
        "            dA = np.dot(dA_curr, W_curr)\n",
        "        else:\n",
        "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
        "            dW = np.dot(A_prev.T, dZ)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True)\n",
        "            dA = np.dot(dZ, W_curr)\n",
        "\n",
        "        return dA, dW, db\n",
        "\n",
        "    def relu_derivative(self, dA, Z):\n",
        "        \"\"\"\n",
        "        ReLU: gradiente de ReLU\n",
        "        \"\"\"\n",
        "        dZ = np.array(dA, copy = True)\n",
        "        dZ[Z <= 0] = 0\n",
        "        return dZ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v1J-6hMeybT-"
      },
      "outputs": [],
      "source": [
        "class RedNeuronal:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.red = [] ## capas\n",
        "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
        "        self.pesos = [] ## W, b\n",
        "        self.memoria = [] ## Z, A\n",
        "        self.gradientes = [] ## dW, db\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def add(self, capa):\n",
        "        \"\"\"\n",
        "        Agregar capa a la red\n",
        "        \"\"\"\n",
        "        self.red.append(capa)\n",
        "\n",
        "    def _compile(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar la arquitectura\n",
        "        \"\"\"\n",
        "        for idx, _ in enumerate(self.red):\n",
        "            if idx == 0:\n",
        "                self.arquitectura.append({'input_dim': data.shape[1], #data es el input inicial\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            elif idx > 0 and idx < len(self.red)-1:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            else:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'softmax'})\n",
        "        return self\n",
        "\n",
        "    def _init_weights(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar arquitectura y los pesos\n",
        "        \"\"\"\n",
        "        self._compile(data)\n",
        "\n",
        "        np.random.seed(99)\n",
        "\n",
        "        for i in range(len(self.arquitectura)):\n",
        "            self.pesos.append({\n",
        "                'W':np.random.uniform(low=-1, high=1,\n",
        "                        size=(self.arquitectura[i]['input_dim'],\n",
        "                            self.arquitectura[i]['output_dim']\n",
        "                            )),\n",
        "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _forwardprop(self, data):\n",
        "        \"\"\"\n",
        "        Pasada forward completa por la red\n",
        "        \"\"\"\n",
        "        A_curr = data\n",
        "\n",
        "        for i in range(len(self.pesos)):\n",
        "            A_prev = A_curr\n",
        "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev,\n",
        "                                                    weights=self.pesos[i]['W'].T,\n",
        "                                                    bias=self.pesos[i]['b'],\n",
        "                                                    activation=self.arquitectura[i]['activation'])\n",
        "\n",
        "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n",
        "\n",
        "        return A_curr\n",
        "\n",
        "    def _backprop(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Pasada backward completa por la red\n",
        "        \"\"\"\n",
        "        num_samples = len(actual)\n",
        "\n",
        "        ## compute the gradient on predictions\n",
        "        dscores = predicted\n",
        "        dscores[range(num_samples),actual] -= 1\n",
        "        dscores /= num_samples\n",
        "\n",
        "        dA_prev = dscores\n",
        "\n",
        "        for idx, layer in reversed(list(enumerate(self.red))):\n",
        "            dA_curr = dA_prev\n",
        "\n",
        "            A_prev = self.memoria[idx]['inputs']\n",
        "            Z_curr = self.memoria[idx]['Z']\n",
        "            W_curr = self.pesos[idx]['W']\n",
        "\n",
        "            activation = self.arquitectura[idx]['activation']\n",
        "\n",
        "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n",
        "\n",
        "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n",
        "\n",
        "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n",
        "\n",
        "    def _update(self):\n",
        "        \"\"\"\n",
        "        Actualizar el modelo --> lr * gradiente\n",
        "        \"\"\"\n",
        "        lr = self.lr\n",
        "        for idx, layer in enumerate(self.red):\n",
        "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n",
        "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n",
        "\n",
        "    def _get_accuracy(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calcular accuracy después de cada iteración\n",
        "        \"\"\"\n",
        "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
        "\n",
        "    def _calculate_loss(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calculate cross-entropy loss after each iteration\n",
        "        \"\"\"\n",
        "        samples = len(actual)\n",
        "\n",
        "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
        "        data_loss = np.sum(correct_logprobs)/samples\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    def train(self, X_train, y_train, epochs):\n",
        "        \"\"\"\n",
        "        Entrenar el modelo Stochastic Gradient Descent\n",
        "        \"\"\"\n",
        "        self.loss = []\n",
        "        self.accuracy = []\n",
        "\n",
        "        self._init_weights(X_train)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            yhat = self._forwardprop(X_train)\n",
        "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
        "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
        "\n",
        "            self._backprop(predicted=yhat, actual=y_train)\n",
        "\n",
        "            self._update()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
        "                print(s)\n",
        "\n",
        "        return (self.accuracy, self.loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcVC3w54ybT_"
      },
      "source": [
        "Los items que se presentan a continuación tienen como objetivo explorar las clases que componen la red neuronal propuesta, comprender su arquitectura y funcionamiento.\n",
        "\n",
        "Nuevamente, lo ideal es no mirar todos los métodos hasta que llegue el momento de utilizarlos.\n",
        "\n",
        "1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la última, utilizando los métodos `add()`, `_compile()` de la clase `RedNeuronal` y el constructor de la clase `Capa`.\n",
        "  \n",
        "    Imprimir la arquitectura del modelo y asegurarse de obtener:\n",
        "\n",
        "    ```\n",
        "    [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n",
        "    {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n",
        "    {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n",
        "    {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\n",
        "    ```\n",
        "\n",
        "    Dibujar la red en papel.\n",
        "\n",
        "1. Inicializar los pesos de la red del punto anterior (`_init_weights(datos)`) y verificar que los pesos tienen dimensión correcta:\n",
        "\n",
        "    ```\n",
        "    capa 0: w=(4, 6) - b=(1, 6)\n",
        "    capa 1: w=(6, 8) - b=(1, 8)\n",
        "    capa 2: w=(8, 10) - b=(1, 10)\n",
        "    capa 3: w=(10, 3) - b=(1, 3)\n",
        "    ```\n",
        "\n",
        "    Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas. Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n",
        "\n",
        "    $$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\n",
        "\n",
        "1. Funciones de activación de una `Capa`:\n",
        "\n",
        "    1. Verificar que el funcionamiento de `ReLU` se corresponda con:\n",
        "\n",
        "        ```\n",
        "        if input > 0:\n",
        "            return input\n",
        "        else:\n",
        "            return 0\n",
        "        ```\n",
        "\n",
        "    1. Verificar que el funcionamiento de `softmax` se corresponda con:\n",
        "\n",
        "        $$\\sigma(Z)_i = \\frac{e^{z_i}}{\\sum_{i=1}^{n} e^{z_j}}$$\n",
        "\n",
        "    **Nota**: para probar estos dos métodos puede ser util construir un vector de la siguiente manera: `np.array([[1.3, 5.1, -2.2, 0.7, 1.1]])` que genera un vector de tamaño (1,5).\n",
        "\n",
        "1. Avancemos con `_forwardprop(datos)`, si corremos la red inicializada con los datos:\n",
        "\n",
        "    1. ¿Qué nos tipo de objeto nos devuelve este método?\n",
        "\n",
        "    1. ¿Qué quiere decir cada uno de los valores?\n",
        "\n",
        "    1. La primera fila, que se correspondería con la primera observación del dataset, ¿qué resultados nos da?¿qué es más probable: 'setosa', 'versicolor' o 'virginica'?¿qué valor es el real?¿por qué?\n",
        "\n",
        "1. Arrancamos a propagar para atrás lo aprendido en la primera pasada. Esto lo realizaremos con el método `_backprop`.\n",
        "\n",
        "    1. ¿Cómo es la derivada de la función de activación `ReLU`?¿Su código es correcto?\n",
        "\n",
        "    1. ¿Cuál es la operación matemática que hace la función `backward` de la clase `Capa` en el caso de tener como activación a `relu`?\n",
        "\n",
        "    1. El método `_backprop` toma 2 parámetros: `predicted` y `actual`. ¿qué debemos pasarle en dicho lugar?\n",
        "\n",
        "        Si la respuesta no fue: en `predicted` le pasamos el resultado de `_forwardprop(...)` y en `actual` le pasamos `y`.... volver a pensarlo. ;-)\n",
        "\n",
        "    1. Verificar que los `gradientes` y los `pesos` para cada una de las capas tienen el mismo tamaño.\n",
        "\n",
        "1. Preparemos por último las funciones necesarias para el entrenamiento. Describir brevemente qué hacen las funciones:\n",
        "\n",
        "    - `_get_accuracy`\n",
        "    - `_calculate_loss`\n",
        "    - `_update`\n",
        "\n",
        "1. Incluyamos finalmente la función `train` y entrenemos una red con la arquitectura propuesta en el punto 1 por 200 epocas.\n",
        "\n",
        "    1. ¿Qué valores se imprimen?¿Qué es posible interpretar de ellos?\n",
        "\n",
        "    1. Graficar el _accuracy_ y la _loss_ que arroja el entramiento en función de las _epochs_. ¿Qué se puede concluir? Probablemente la señal sea ruidosa, por lo que se recomienda hacer un suavizado por ventanas deslizantes.\n",
        "\n",
        "1. Reimplementar la clase `RedNeuronal` utilizando PyTorch\n",
        "\n",
        "    Hasta ahora hemos construido nuestra propia red neuronal \"desde cero\", lo cual nos permitió comprender en profundidad cómo funciona cada componente: inicialización de pesos, funciones de activación, forward y backward propagation, cálculo de loss y accuracy, y actualización de pesos.\n",
        "\n",
        "    Sin embargo, en proyectos reales y más complejos, utilizamos frameworks como **PyTorch** que abstraen estas tareas, permitiéndonos enfocarnos más en el diseño de la arquitectura y el análisis de los resultados.  \n",
        "\n",
        "    **Objetivo de este inciso**: recrear la arquitectura y entrenamiento de nuestra red neuronal, pero usando herramientas provistas por PyTorch. Esto implica:\n",
        "\n",
        "    1. Implementar una clase `RedNeuronalTorch` que herede de `nn.Module` y contenga una red con la misma arquitectura:  \n",
        "    - Entrada de dimensión 4 (por las características del dataset Iris)\n",
        "    - Capas ocultas de 6, 8 y 10 nodos respectivamente\n",
        "    - Capa de salida con 3 nodos y activación `softmax`\n",
        "\n",
        "    2. Entrenar esta nueva red por 200 épocas utilizando:\n",
        "    - Función de pérdida: `nn.CrossEntropyLoss`\n",
        "    - Optimizador: `torch.optim.SGD`\n",
        "    - Tasa de aprendizaje: 0.01\n",
        "\n",
        "    3. Comparar los resultados obtenidos con los del entrenamiento anterior (implementación manual). Algunas preguntas a responder:\n",
        "    - ¿La convergencia es más rápida o más lenta?\n",
        "    - ¿Cómo se comporta la pérdida durante el entrenamiento?\n",
        "    - ¿Cuál implementación fue más fácil de modificar o extender?\n",
        "\n",
        "    4. Graficar la evolución de la **pérdida** y el **accuracy** durante las épocas para ambas implementaciones (manual y PyTorch), idealmente en la misma figura para facilitar la comparación. Podés aplicar una media móvil para suavizar la señal.\n",
        "\n",
        "    > 💡 **Sugerencia pedagógica**: antes de realizar este inciso, se recomienda repasar los notebooks `9a` y `9b`, donde se presentan una introducción a los tensores y al workflow de ML usando PyTorch.\n",
        "\n",
        "\n",
        "Crédito: este ejercicio se base en la propuesta de Joe Sasson publicada en [Towards Data Science](https://towardsdatascience.com/coding-a-neural-network-from-scratch-in-numpy-31f04e4d605)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"1. Crear una Red Neuronal con 6 nodos en la primera capa, 8 en la segunda, 10 en la tercer y finalmente 3 en la última, utilizando los métodos\n",
        "add(), _compile() de la clase RedNeuronal y el constructor de la clase Capa.\n",
        "\n",
        "Imprimir la arquitectura del modelo y asegurarse de obtener:\n",
        "\n",
        " [{'input_dim': 4, 'output_dim': 6, 'activation': 'relu'},\n",
        " {'input_dim': 6, 'output_dim': 8, 'activation': 'relu'},\n",
        " {'input_dim': 8, 'output_dim': 10, 'activation': 'relu'},\n",
        " {'input_dim': 10, 'output_dim': 3, 'activation': 'softmax'}]\"\"\"\n",
        "\n",
        "nueva_red = RedNeuronal()\n",
        "nueva_red.add(Capa(neuronas=6))\n",
        "nueva_red.add(Capa(neuronas=8))\n",
        "nueva_red.add(Capa(neuronas=10))\n",
        "nueva_red.add(Capa(neuronas=3))\n",
        "#nueva_red._compile(data=X)\n",
        "\n",
        "#nueva_red.arquitectura\n",
        "\n",
        "#ej 1 correcto"
      ],
      "metadata": {
        "id": "Pgwz8ggPMUz0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Inicializar los pesos de la red del punto anterior (_init_weights(datos)) y verificar que los pesos tienen dimensión correcta:\n",
        "\n",
        " capa 0: w=(4, 6) - b=(1, 6)\n",
        " capa 1: w=(6, 8) - b=(1, 8)\n",
        " capa 2: w=(8, 10) - b=(1, 10)\n",
        " capa 3: w=(10, 3) - b=(1, 3)\n",
        "Definir las matrices que se corresponden con las capas de manera que una pasada pueda ser interpretada como el producto de todas ellas.\n",
        "Recordar que en cada paso por cada capa estaremos computando por cada neurona de la capa siguiente:\n",
        "\n",
        "$$Z = \\sum_{i=1}^{n} X_i \\times W_i + b$$\"\"\"\n",
        "\n",
        "nueva_red._init_weights(X)\n",
        "#nueva_red.pesos"
      ],
      "metadata": {
        "id": "21a9VbQYOk0S",
        "outputId": "261621bc-9113-4eb2-e8e1-c087385d4302",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RedNeuronal at 0x7a65d4c12cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Avancemos con _forwardprop(datos), si corremos la red inicializada con los datos:\n",
        "\n",
        "¿Qué nos tipo de objeto nos devuelve este método? Devuelve una matriz de 150 filas y 3 columnas.\n",
        "\n",
        "¿Qué quiere decir cada uno de los valores? Cada fila da las probabilidades estimadas de que cada instancia pertenezca a cada clase (luego de una iteración)\n",
        "\n",
        "La primera fila, que se correspondería con la primera observación del dataset, ¿qué resultados nos da?¿qué es más probable: el más probable es el segundo,\n",
        "pero sólo hicimos una iteración. Por eso, la clase real es la 0.\n",
        "'setosa', 'versicolor' o 'virginica'?¿qué valor es el real?¿por qué?\"\"\"\n",
        "\n",
        "nueva_red._forwardprop(X)"
      ],
      "metadata": {
        "id": "RTA74ZaFQ5M7",
        "outputId": "de6cd3ba-c705-44b4-b992-5584944cfe2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16617107, 0.81859653, 0.0152324 ],\n",
              "       [0.18455954, 0.79567724, 0.01976322],\n",
              "       [0.18822018, 0.7910944 , 0.02068542],\n",
              "       [0.18491961, 0.79505216, 0.02002823],\n",
              "       [0.16586973, 0.81894977, 0.0151805 ],\n",
              "       [0.1934376 , 0.79413117, 0.01243122],\n",
              "       [0.21199684, 0.76713852, 0.02086464],\n",
              "       [0.16711898, 0.81733696, 0.01554407],\n",
              "       [0.20016985, 0.77546863, 0.02436152],\n",
              "       [0.15038117, 0.83343343, 0.0161854 ],\n",
              "       [0.15052877, 0.83744754, 0.01202369],\n",
              "       [0.16776631, 0.81642573, 0.01580796],\n",
              "       [0.1596101 , 0.8218913 , 0.0184986 ],\n",
              "       [0.18239647, 0.79199777, 0.02560577],\n",
              "       [0.14576733, 0.84331948, 0.01091319],\n",
              "       [0.18006735, 0.80965058, 0.01028207],\n",
              "       [0.2121044 , 0.77257833, 0.01531726],\n",
              "       [0.19555794, 0.78750755, 0.01693451],\n",
              "       [0.15989536, 0.82965933, 0.01044531],\n",
              "       [0.18247077, 0.80319089, 0.01433834],\n",
              "       [0.15041758, 0.83745092, 0.0121315 ],\n",
              "       [0.21720193, 0.76630521, 0.01649286],\n",
              "       [0.19241133, 0.78606076, 0.02152791],\n",
              "       [0.25614725, 0.72487494, 0.01897781],\n",
              "       [0.15601521, 0.83051551, 0.01346927],\n",
              "       [0.17371827, 0.80909912, 0.0171826 ],\n",
              "       [0.22481917, 0.7570559 , 0.01812493],\n",
              "       [0.15994116, 0.82611572, 0.01394312],\n",
              "       [0.16647282, 0.81824273, 0.01528446],\n",
              "       [0.17549118, 0.80683789, 0.01767093],\n",
              "       [0.17580451, 0.8064647 , 0.01773079],\n",
              "       [0.21830831, 0.76503517, 0.01665651],\n",
              "       [0.12207744, 0.86808931, 0.00983325],\n",
              "       [0.14015659, 0.84976629, 0.01007712],\n",
              "       [0.17755258, 0.80439476, 0.01805266],\n",
              "       [0.18502839, 0.79531739, 0.01965422],\n",
              "       [0.16094322, 0.8250945 , 0.01396228],\n",
              "       [0.14216003, 0.8437743 , 0.01406567],\n",
              "       [0.20172805, 0.77356073, 0.02471123],\n",
              "       [0.16480185, 0.82018912, 0.01500903],\n",
              "       [0.20276653, 0.7787703 , 0.01846318],\n",
              "       [0.26003131, 0.70529162, 0.03467707],\n",
              "       [0.19585647, 0.78121474, 0.0229288 ],\n",
              "       [0.2971502 , 0.68194985, 0.02089995],\n",
              "       [0.19525149, 0.79185041, 0.0128981 ],\n",
              "       [0.21898122, 0.75839522, 0.02262355],\n",
              "       [0.15085307, 0.83695954, 0.01218739],\n",
              "       [0.1864018 , 0.79327782, 0.02032039],\n",
              "       [0.15270127, 0.83484185, 0.01245688],\n",
              "       [0.17385148, 0.80911815, 0.01703037],\n",
              "       [0.39091341, 0.60412662, 0.00495997],\n",
              "       [0.479359  , 0.51372964, 0.00691136],\n",
              "       [0.43400692, 0.56086635, 0.00512673],\n",
              "       [0.49594704, 0.49031225, 0.0137407 ],\n",
              "       [0.48695648, 0.50573299, 0.00731053],\n",
              "       [0.42874366, 0.56246734, 0.00878901],\n",
              "       [0.51414026, 0.47926428, 0.00659546],\n",
              "       [0.41787148, 0.56344476, 0.01868376],\n",
              "       [0.38036012, 0.61351505, 0.00612483],\n",
              "       [0.54597636, 0.43970926, 0.01431438],\n",
              "       [0.41813695, 0.56295566, 0.01890739],\n",
              "       [0.53148878, 0.45878784, 0.00972338],\n",
              "       [0.33709071, 0.65271005, 0.01019924],\n",
              "       [0.44218256, 0.55055867, 0.00725876],\n",
              "       [0.49233137, 0.49456814, 0.01310049],\n",
              "       [0.42923999, 0.56428292, 0.00647709],\n",
              "       [0.52247783, 0.46819301, 0.00932916],\n",
              "       [0.31711743, 0.67420373, 0.00867884],\n",
              "       [0.53509587, 0.45481035, 0.01009378],\n",
              "       [0.39219442, 0.59607449, 0.01173108],\n",
              "       [0.62528895, 0.36697383, 0.00773721],\n",
              "       [0.44780453, 0.542433  , 0.00976248],\n",
              "       [0.4879185 , 0.50458884, 0.00749266],\n",
              "       [0.35140256, 0.64198534, 0.0066121 ],\n",
              "       [0.40944581, 0.58304857, 0.00750562],\n",
              "       [0.43812621, 0.5549671 , 0.00690669],\n",
              "       [0.41062048, 0.58360069, 0.00577884],\n",
              "       [0.53953807, 0.45456794, 0.00589399],\n",
              "       [0.51048555, 0.48094395, 0.00857051],\n",
              "       [0.36415795, 0.62343953, 0.01240252],\n",
              "       [0.4075882 , 0.57929845, 0.01311335],\n",
              "       [0.36730985, 0.61981048, 0.01287967],\n",
              "       [0.42299546, 0.566053  , 0.01095154],\n",
              "       [0.52634857, 0.46631849, 0.00733293],\n",
              "       [0.53078607, 0.45932861, 0.00988533],\n",
              "       [0.5364806 , 0.45592226, 0.00759714],\n",
              "       [0.45691688, 0.53711541, 0.00596771],\n",
              "       [0.4344037 , 0.55654688, 0.00904943],\n",
              "       [0.45202444, 0.53781246, 0.0101631 ],\n",
              "       [0.48665819, 0.50045596, 0.01288585],\n",
              "       [0.40478478, 0.58534497, 0.00987025],\n",
              "       [0.44465011, 0.54800303, 0.00734686],\n",
              "       [0.42058032, 0.5686012 , 0.01081848],\n",
              "       [0.41838798, 0.56287526, 0.01873676],\n",
              "       [0.45893476, 0.53033825, 0.01072699],\n",
              "       [0.39235847, 0.59876077, 0.00888076],\n",
              "       [0.4454281 , 0.54482938, 0.00974252],\n",
              "       [0.417661  , 0.57435334, 0.00798566],\n",
              "       [0.47452406, 0.50497287, 0.02050306],\n",
              "       [0.45724657, 0.53222045, 0.01053298],\n",
              "       [0.81339647, 0.18232411, 0.00427942],\n",
              "       [0.67585404, 0.31586641, 0.00827956],\n",
              "       [0.65181867, 0.34417197, 0.00400937],\n",
              "       [0.56714861, 0.42738354, 0.00546785],\n",
              "       [0.72250708, 0.27258802, 0.00490491],\n",
              "       [0.58314422, 0.41422351, 0.00263227],\n",
              "       [0.66624159, 0.31997916, 0.01377925],\n",
              "       [0.47336902, 0.52361671, 0.00301427],\n",
              "       [0.55493435, 0.43998631, 0.00507934],\n",
              "       [0.77482377, 0.22211597, 0.00306026],\n",
              "       [0.67328693, 0.3207098 , 0.00600327],\n",
              "       [0.64057192, 0.35290426, 0.00652382],\n",
              "       [0.68872711, 0.30620182, 0.00507108],\n",
              "       [0.73365585, 0.25700239, 0.00934176],\n",
              "       [0.84530771, 0.14707592, 0.00761637],\n",
              "       [0.78204175, 0.21229468, 0.00566357],\n",
              "       [0.56126382, 0.43351448, 0.0052217 ],\n",
              "       [0.58272215, 0.41530656, 0.00197128],\n",
              "       [0.67010659, 0.3272419 , 0.00265151],\n",
              "       [0.50755568, 0.48381295, 0.00863137],\n",
              "       [0.74646383, 0.24920637, 0.0043298 ],\n",
              "       [0.73155948, 0.25928945, 0.00915107],\n",
              "       [0.53172897, 0.46574271, 0.00252832],\n",
              "       [0.62500685, 0.36726026, 0.00773289],\n",
              "       [0.66724963, 0.32832822, 0.00442215],\n",
              "       [0.48524004, 0.51154219, 0.00321777],\n",
              "       [0.63119524, 0.36077705, 0.0080277 ],\n",
              "       [0.61955898, 0.37297121, 0.00746981],\n",
              "       [0.70479368, 0.28946874, 0.00573758],\n",
              "       [0.41036663, 0.58625104, 0.00338233],\n",
              "       [0.53873496, 0.45782419, 0.00344085],\n",
              "       [0.49662403, 0.50140607, 0.0019699 ],\n",
              "       [0.74436247, 0.24991306, 0.00572447],\n",
              "       [0.45908061, 0.5347061 , 0.00621329],\n",
              "       [0.39219841, 0.60250115, 0.00530044],\n",
              "       [0.70398234, 0.29280504, 0.00321262],\n",
              "       [0.79729356, 0.19782855, 0.0048779 ],\n",
              "       [0.56072179, 0.43406988, 0.00520833],\n",
              "       [0.63018756, 0.36182312, 0.00798932],\n",
              "       [0.68725094, 0.30776203, 0.00498703],\n",
              "       [0.79566197, 0.19954869, 0.00478934],\n",
              "       [0.78086395, 0.21361699, 0.00551906],\n",
              "       [0.67585404, 0.31586641, 0.00827956],\n",
              "       [0.73861355, 0.25725887, 0.00412758],\n",
              "       [0.81632455, 0.17936676, 0.00430869],\n",
              "       [0.78479264, 0.20945933, 0.00574803],\n",
              "       [0.67218094, 0.31983457, 0.00798449],\n",
              "       [0.67526303, 0.31861797, 0.006119  ],\n",
              "       [0.77641254, 0.21814369, 0.00544377],\n",
              "       [0.6138536 , 0.37886426, 0.00728214]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyEx4crsybUB"
      },
      "source": [
        "### Código completo (Implementación con Numpy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jVZnVoQSybUC"
      },
      "outputs": [],
      "source": [
        "class Capa:\n",
        "    def __init__(self, neuronas):\n",
        "        self.neuronas = neuronas\n",
        "\n",
        "    def forward(self, inputs, weights, bias, activation):\n",
        "        \"\"\"\n",
        "        Forward Propagation de la capa\n",
        "        \"\"\"\n",
        "        Z_curr = np.dot(inputs, weights.T) + bias\n",
        "\n",
        "        if activation == 'relu':\n",
        "            A_curr = self.relu(inputs=Z_curr)\n",
        "        elif activation == 'softmax':\n",
        "            A_curr = self.softmax(inputs=Z_curr)\n",
        "\n",
        "        return A_curr, Z_curr\n",
        "\n",
        "    def relu(self, inputs):\n",
        "        \"\"\"\n",
        "        ReLU: función de activación\n",
        "        \"\"\"\n",
        "\n",
        "        return np.maximum(0, inputs)\n",
        "\n",
        "    def softmax(self, inputs):\n",
        "        \"\"\"\n",
        "        Softmax: función de activación\n",
        "        \"\"\"\n",
        "        exp_scores = np.exp(inputs)\n",
        "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
        "        return probs\n",
        "\n",
        "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
        "        \"\"\"\n",
        "        Backward Propagation de la capa\n",
        "        \"\"\"\n",
        "        if activation == 'softmax':\n",
        "            dW = np.dot(A_prev.T, dA_curr)\n",
        "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
        "            dA = np.dot(dA_curr, W_curr)\n",
        "        else:\n",
        "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
        "            dW = np.dot(A_prev.T, dZ)\n",
        "            db = np.sum(dZ, axis=0, keepdims=True)\n",
        "            dA = np.dot(dZ, W_curr)\n",
        "\n",
        "        return dA, dW, db\n",
        "\n",
        "    def relu_derivative(self, dA, Z):\n",
        "        \"\"\"\n",
        "        ReLU: gradiente de ReLU\n",
        "        \"\"\"\n",
        "        dZ = np.array(dA, copy = True)\n",
        "        dZ[Z <= 0] = 0\n",
        "        return dZ\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjSDa2Y4ybUD"
      },
      "outputs": [],
      "source": [
        "class RedNeuronal:\n",
        "    def __init__(self, learning_rate=0.01):\n",
        "        self.red = [] ## capas\n",
        "        self.arquitectura = [] ## mapeo de entradas -> salidas\n",
        "        self.pesos = [] ## W, b\n",
        "        self.memoria = [] ## Z, A\n",
        "        self.gradientes = [] ## dW, db\n",
        "        self.lr = learning_rate\n",
        "\n",
        "    def add(self, capa):\n",
        "        \"\"\"\n",
        "        Agregar capa a la red\n",
        "        \"\"\"\n",
        "        self.red.append(capa)\n",
        "\n",
        "    def _compile(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar la arquitectura\n",
        "        \"\"\"\n",
        "        for idx, _ in enumerate(self.red):\n",
        "            if idx == 0:\n",
        "                self.arquitectura.append({'input_dim': data.shape[1],\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            elif idx > 0 and idx < len(self.red)-1:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'relu'})\n",
        "            else:\n",
        "                self.arquitectura.append({'input_dim': self.red[idx-1].neuronas,\n",
        "                                        'output_dim': self.red[idx].neuronas,\n",
        "                                        'activation':'softmax'})\n",
        "        return self\n",
        "\n",
        "    def _init_weights(self, data):\n",
        "        \"\"\"\n",
        "        Inicializar arquitectura y los pesos\n",
        "        \"\"\"\n",
        "        self._compile(data)\n",
        "\n",
        "        np.random.seed(99)\n",
        "\n",
        "        for i in range(len(self.arquitectura)):\n",
        "            self.pesos.append({\n",
        "                'W':np.random.uniform(low=-1, high=1,\n",
        "                        size=(self.arquitectura[i]['input_dim'],\n",
        "                            self.arquitectura[i]['output_dim']\n",
        "                            )),\n",
        "                'b':np.zeros((1, self.arquitectura[i]['output_dim']))})\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _forwardprop(self, data):\n",
        "        \"\"\"\n",
        "        Pasada forward completa por la red\n",
        "        \"\"\"\n",
        "        A_curr = data\n",
        "\n",
        "        for i in range(len(self.pesos)):\n",
        "            A_prev = A_curr\n",
        "            A_curr, Z_curr = self.red[i].forward(inputs=A_prev,\n",
        "                                                    weights=self.pesos[i]['W'].T,\n",
        "                                                    bias=self.pesos[i]['b'],\n",
        "                                                    activation=self.arquitectura[i]['activation'])\n",
        "\n",
        "            self.memoria.append({'inputs':A_prev, 'Z':Z_curr})\n",
        "\n",
        "        return A_curr\n",
        "\n",
        "    def _backprop(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Pasada backward completa por la red\n",
        "        \"\"\"\n",
        "        num_samples = len(actual)\n",
        "\n",
        "        ## compute the gradient on predictions\n",
        "        dscores = predicted\n",
        "        dscores[range(num_samples),actual] -= 1\n",
        "        dscores /= num_samples\n",
        "\n",
        "        dA_prev = dscores\n",
        "\n",
        "        for idx, layer in reversed(list(enumerate(self.red))):\n",
        "            dA_curr = dA_prev\n",
        "\n",
        "            A_prev = self.memoria[idx]['inputs']\n",
        "            Z_curr = self.memoria[idx]['Z']\n",
        "            W_curr = self.pesos[idx]['W']\n",
        "\n",
        "            activation = self.arquitectura[idx]['activation']\n",
        "\n",
        "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr.T, Z_curr, A_prev, activation)\n",
        "\n",
        "            self.gradientes.append({'dW':dW_curr, 'db':db_curr})\n",
        "\n",
        "        self.gradientes = list(reversed(self.gradientes))  # Reverse the gradients list\n",
        "\n",
        "    def _update(self):\n",
        "        \"\"\"\n",
        "        Actualizar el modelo --> lr * gradiente\n",
        "        \"\"\"\n",
        "        lr = self.lr\n",
        "        for idx, layer in enumerate(self.red):\n",
        "            self.pesos[idx]['W'] -= lr * self.gradientes[idx]['dW']\n",
        "            self.pesos[idx]['b'] -= lr * self.gradientes[idx]['db']\n",
        "\n",
        "    def _get_accuracy(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calcular accuracy después de cada iteración\n",
        "        \"\"\"\n",
        "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
        "\n",
        "    def _calculate_loss(self, predicted, actual):\n",
        "        \"\"\"\n",
        "        Calculate cross-entropy loss after each iteration\n",
        "        \"\"\"\n",
        "        samples = len(actual)\n",
        "\n",
        "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
        "        data_loss = np.sum(correct_logprobs)/samples\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    def train(self, X_train, y_train, epochs):\n",
        "        \"\"\"\n",
        "        Entrenar el modelo Stochastic Gradient Descent\n",
        "        \"\"\"\n",
        "        self.loss = []\n",
        "        self.accuracy = []\n",
        "\n",
        "        self._init_weights(X_train)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            yhat = self._forwardprop(X_train)\n",
        "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
        "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
        "\n",
        "            self._backprop(predicted=yhat, actual=y_train)\n",
        "\n",
        "            self._update()\n",
        "\n",
        "            if i % 20 == 0:\n",
        "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
        "                print(s)\n",
        "\n",
        "        return (self.accuracy, self.loss)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}